# Enabled LLM Providers (e.g. Localhost,ChatGLM,GPT4ALL,LlamaCPP,Ollama,XInference,Huggingface,OpenLM,OpenLLM,OpenAI)
LLM_PROVIDERS=LlamaCPP,GPT4ALL,Huggingface,Localhost,ChatGLM,Ollama,XInference,OpenLM,OpenLLM,OpenAI

# Enabled Vector Databases e.g. (Chroma,DeepLake)
VECTOR_DB_PROVIDERS=Chroma,DeepLake

# Set to TRUE to enable verbose mode, providing detailed output, or FALSE to disable it.
VERBOSE=TRUE

# Strip comments from texts to prevent get answers about licenses and copyright.
REMOVE_COMMENTS=TRUE

# Use a prompt for the QA Retrieval
PROMPT_ENABLED=FALSE

# Set to TRUE if you want to use a GPU for processing, or FALSE for CPU.
GPU_ENABLED=FALSE

# Specify the CPU percentage for system resources. Set to 0.8 for 80% CPU utilization.
CPU_PERCENTAGE=0.8

# Specify the number of target source chunks for document retrieval.
TARGET_SOURCE_CHUNKS=10

# Define the directory where persistent data will be stored.
DOCUMENT_PATH=documents

# Specify the path to the models directory.
MODEL_PATH=models

# Define the directory where persistent data will be stored.
DB_PATH=db

# Set the name of the embeddings model to use. (e.g. all-MiniLM-L6-v2, gtr-t5-large, sentence-t5-xl)
# The all-MiniLM-L6-v2 model is the fastest, others may take long to embed.
# Embeddings leaderboards https://huggingface.co/spaces/mteb/leaderboard
EMBEDDINGS_MODEL_NAME=all-MiniLM-L6-v2

# The default LLM (used for skipping first 2 steps)
DEFAULT_LLM=llamacpp

# The default Vector Database (used for skipping first 2 steps)
DEFAULT_VECTOR_DB=chroma

# The default Github repository (When prompt ask for repo you can just hit enter)
GITHUB_REPOSITORY=https://github.com/jsonjuri/php-syntax-color-highlighter

# For private repositories fill in your username and api key.
# Create a classic token on https://github.com/settings/tokens
GITHUB_USERNAME=
GITHUB_ACCESS_TOKEN=
