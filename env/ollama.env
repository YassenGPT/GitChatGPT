# https://python.langchain.com/docs/integrations/llms/ollama

# Any Ollama model tag (e.g. vicuna, mistral, codellama, or llama2-uncensored)
OLLAMA_LLM=llama2

# Localhost Webserver
OLLAMA_BASE_URL=http://127.0.0.1:11434

# Define the specific model to use within the 'MODEL_PATH'.
MODEL=codellama-7b.Q5_K_M.gguf

# The context size is the maximum number of tokens that the model can account for when processing a response.
MODEL_MAX_TOKENS=2048

# Set the temperature for the model.
# Influences the randomness of the model's output.
MODEL_TEMPERATURE=0.75

# Reduces the probability of generating nonsense.
# A higher value (e.g. 100) will give more diverse answers,
# while a lower value (e.g. 10) will be more conservative. (Default: 40)
MODEL_TOP_K=0.9

# Set the top-p value for the model.
# Helping control randomness and potentially improving coherence in generated text.
MODEL_TOP_P=0.9