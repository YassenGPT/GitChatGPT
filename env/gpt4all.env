# https://python.langchain.com/docs/integrations/llms/gpt4all

# https://gpt4all.io/models/ggml-gpt4all-j-v1.3-groovy.bin
# https://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin
# Define the specific model to use within the 'MODEL_PATH'.
MODEL=ggml-gpt4all-j.bin

# Whether to stream the results or not.
MODEL_STREAMING=FALSE

# Force system to keep model in RAM.
MODEL_USE_MLOCK=TRUE

# The context size is the maximum number of tokens that the model can account for when processing a response.
MODEL_MAX_TOKENS=2048

# Set the temperature for the model.
# Influences the randomness of the model's output.
MODEL_TEMPERATURE=0.75

# Reduces the probability of generating nonsense.
# A higher value (e.g. 100) will give more diverse answers,
# while a lower value (e.g. 10) will be more conservative. (Default: 40)
MODEL_TOP_K=0.9

# Set the top-p value for the model.
# Helping control randomness and potentially improving coherence in generated text.
MODEL_TOP_P=0.9