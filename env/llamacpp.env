# https://python.langchain.com/docs/integrations/llms/llamacpp

# https://huggingface.co/TheBloke/Xwin-LM-7B-V0.1-GGUF/resolve/main/xwin-lm-7b-v0.1.Q5_K_M.gguf?download=true
# Define the specific model to use within the 'MODEL_PATH'.
MODEL=xwin-lm-7b-v0.1.Q5_K_M.gguf

# Force system to keep model in RAM.
MODEL_USE_MLOCK=TRUE

# The context size is the maximum number of tokens that the model can account for when processing a response.
MODEL_MAX_TOKENS=2048

# Set the temperature for the model.
# Influences the randomness of the model's output.
MODEL_TEMPERATURE=0.75

# Set the top-p value for the model.
# Helping control randomness and potentially improving coherence in generated text.
MODEL_TOP_P=0.9